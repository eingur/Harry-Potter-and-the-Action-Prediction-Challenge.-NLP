{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9hYewOFfp4ng"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import glob\n",
    "from  nltk import word_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import os\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from gensim.models import FastText\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4o8J9xWOqILG",
    "outputId": "a11a8d8a-5613-4c59-8f70-eb749f36ee04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-17 17:38:29--  https://www.dropbox.com/s/aogtja9upycdqv4/hpac_splits.zip?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/raw/aogtja9upycdqv4/hpac_splits.zip [following]\n",
      "--2022-11-17 17:38:29--  https://www.dropbox.com/s/raw/aogtja9upycdqv4/hpac_splits.zip\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com/cd/0/inline/Bw-02CFnWHgGtr0du0A81fRw3HmD1OyHpQrddA1yLKEcuEnF6BzMFFSYe3fTJRNl26PZSsEZ2WACcChXNbFHYCVlY5hlSLVd-pkwb_qqTSANAQq9HXI7d8MCxpxEAcrBslYf36_qbkVx03Bp0W3BRhMzQfCKupM8Mg0cfUMcl0wPlg/file# [following]\n",
      "--2022-11-17 17:38:30--  https://uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com/cd/0/inline/Bw-02CFnWHgGtr0du0A81fRw3HmD1OyHpQrddA1yLKEcuEnF6BzMFFSYe3fTJRNl26PZSsEZ2WACcChXNbFHYCVlY5hlSLVd-pkwb_qqTSANAQq9HXI7d8MCxpxEAcrBslYf36_qbkVx03Bp0W3BRhMzQfCKupM8Mg0cfUMcl0wPlg/file\n",
      "Resolving uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com (uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
      "Connecting to uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com (uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/Bw8v7w1kQdk3-Dr1JXdslYcB5ylH-3HTS-XSWYyKVRbOG0WamcNdMbJ-wkLXru2may0Qo22wrpK06m52rIX5R1xa5vioMdi5_ZIh2Ply7wDtA_UaAfwULSIU6u4MU7XGuWY_zn9cw6m8K6iQuIoulNJCC6e2VIk_gJWbukdfMM0WWlhvh0Zf14JyVtTWRKJw5hasMJVFTF9NuFez1LFmq10NryCktvSiK6GMiHlCPi0oQy4w7-75fNqAFZmRG05q8NAX-ln9N6TMejxhH2_pyqNNK1VnG6V2l1MhXFFAROLTBRI1eUg8VsMlL8u0TrevUAHm87g0ZtlXn7I6-EOeRgm31QYqrh0C46t2HSDU5HudD8xz9lG7C_-DJOfR4eH6LAdtbppbMRvV4qpesfFA0B2Zhq-ea7tgnKf4g4t0vfnY3w/file [following]\n",
      "--2022-11-17 17:38:31--  https://uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com/cd/0/inline2/Bw8v7w1kQdk3-Dr1JXdslYcB5ylH-3HTS-XSWYyKVRbOG0WamcNdMbJ-wkLXru2may0Qo22wrpK06m52rIX5R1xa5vioMdi5_ZIh2Ply7wDtA_UaAfwULSIU6u4MU7XGuWY_zn9cw6m8K6iQuIoulNJCC6e2VIk_gJWbukdfMM0WWlhvh0Zf14JyVtTWRKJw5hasMJVFTF9NuFez1LFmq10NryCktvSiK6GMiHlCPi0oQy4w7-75fNqAFZmRG05q8NAX-ln9N6TMejxhH2_pyqNNK1VnG6V2l1MhXFFAROLTBRI1eUg8VsMlL8u0TrevUAHm87g0ZtlXn7I6-EOeRgm31QYqrh0C46t2HSDU5HudD8xz9lG7C_-DJOfR4eH6LAdtbppbMRvV4qpesfFA0B2Zhq-ea7tgnKf4g4t0vfnY3w/file\n",
      "Reusing existing connection to uc59d7682a7c6d02e06700082660.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18284862 (17M) [application/zip]\n",
      "Saving to: ‘hpac_splits.zip?dl=0’\n",
      "\n",
      "hpac_splits.zip?dl= 100%[===================>]  17.44M  18.4MB/s    in 0.9s    \n",
      "\n",
      "2022-11-17 17:38:32 (18.4 MB/s) - ‘hpac_splits.zip?dl=0’ saved [18284862/18284862]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/aogtja9upycdqv4/hpac_splits.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hTlTcTwqQgz",
    "outputId": "57b5e78d-1e0f-42f6-ed9f-424eab808e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  hpac_splits.zip?dl=0\n",
      "   creating: hpac_corpus/\n",
      "  inflating: hpac_corpus/hpac_training_128.tsv  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/hpac_corpus/\n",
      "  inflating: __MACOSX/hpac_corpus/._hpac_training_128.tsv  \n",
      "  inflating: hpac_corpus/hpac_dev_128.tsv  \n",
      "  inflating: __MACOSX/hpac_corpus/._hpac_dev_128.tsv  \n",
      "  inflating: hpac_corpus/hpac_test_128.tsv  \n",
      "  inflating: __MACOSX/hpac_corpus/._hpac_test_128.tsv  \n",
      "  inflating: __MACOSX/._hpac_corpus  \n"
     ]
    }
   ],
   "source": [
    "!unzip hpac_splits.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LKTW3PPzqSMN"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('hpac_corpus/hpac_training_128.tsv', sep = '\\t', header = None)\n",
    "val_df = pd.read_csv('hpac_corpus/hpac_dev_128.tsv', sep = '\\t', header = None)\n",
    "test_df = pd.read_csv('hpac_corpus/hpac_test_128.tsv', sep = '\\t', header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56jQ5DHjtJDw",
    "outputId": "7d560f9f-0220-4546-905e-b7722912fe76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct = set(list(string.punctuation))\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VlX1obkhqnpg"
   },
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    text = str(text).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punct_sym = string.punctuation\n",
    "    text = text.translate(str.maketrans(\"\",\"\",punct_sym))\n",
    "    text = \" \".join([w for w in text.split() if w not in stop_words])    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEZuhSCXs2L-",
    "outputId": "dd7d959f-93a2-4d1b-bf5a-f97ac198d703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 1 s, total: 11.1 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df[2]=train_df[2].apply(lambda x: format_text(x))\n",
    "val_df[2]=val_df[2].apply(lambda x: format_text(x))\n",
    "test_df[2]=test_df[2].apply(lambda x: format_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AX_wf4SotFM9"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:,1:]\n",
    "train_df.columns = ['label','text']\n",
    "train_df = train_df[['text','label']]\n",
    "\n",
    "val_df = val_df.iloc[:,1:]\n",
    "val_df.columns = ['label','text']\n",
    "val_df = val_df[['text','label']]\n",
    "\n",
    "test_df = test_df.iloc[:,1:]\n",
    "test_df.columns = ['label','text']\n",
    "test_df = test_df[['text','label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "CjYmm1kgSGi-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iaF9EB21OxZc"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import spacy\n",
    "from torchtext.legacy import data \n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = lambda sent: [x.text for x in nlp.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.LabelField()\n",
    "datafields = [(\"text\",TEXT),(\"label\",LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "apkzbtAXOyFV"
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/nissan/ccb0553edb6abafd20c3dec34ee8099d\n",
    "class DataFrameDataset(torchtext.legacy.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        # examples = []\n",
    "        # for i, row in tqdm(df.iterrows()):\n",
    "        #     label = row.label if not is_test else None\n",
    "        #     text = row.text\n",
    "        #     examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        # super().__init__(examples, fields, **kwargs)\n",
    "        examples = [data.Example.fromlist(i, fields) for i in df.values.tolist()]\n",
    "        tdata = data.Dataset(examples, fields)\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), text_field, label_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)\n",
    "        \n",
    "train_ds, val_ds, test_ds = DataFrameDataset.splits(\n",
    "  text_field=TEXT, label_field=LABEL, train_df=train_df, val_df=val_df, test_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tWO-Tzz5lCtt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JLKaf6zVO255"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_ds, max_size=25000)\n",
    "LABEL.build_vocab(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "O4MhSQIFtoaL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n,\n",
    "                 emb_dim,\n",
    "                 input_channel,\n",
    "                 output_channel,\n",
    "                 num_classes):\n",
    "        super().__init__()\n",
    "        self.embs = nn.Embedding(n, emb_dim)\n",
    "        self.conv1 = nn.Conv2d( input_channel, output_channel, kernel_size=(2, emb_dim))\n",
    "            \n",
    "        self.conv2 = nn.Conv2d( input_channel, output_channel, kernel_size=(3, emb_dim))\n",
    "\n",
    "        self.conv3 = nn.Conv2d( input_channel, output_channel, kernel_size=(4, emb_dim))\n",
    "\n",
    "        self.conv4 = nn.Conv2d( input_channel, output_channel, kernel_size=(5, emb_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.linear = nn.Linear(3 * output_channel, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embs(x.t()).unsqueeze(1)\n",
    "        relu1 = F.relu(self.conv1(embeddings).squeeze(3))\n",
    "        relu2 = F.relu(self.conv2(embeddings).squeeze(3))\n",
    "        relu3 = F.relu(self.conv3(embeddings).squeeze(3))\n",
    "        relu4 = F.relu(self.conv4(embeddings).squeeze(3))\n",
    "\n",
    "\n",
    "        layer_1 = F.max_pool1d(relu1, relu1.size(2)).squeeze(2)\n",
    "        layer_2 = F.max_pool1d(relu2, relu2.size(2)).squeeze(2)\n",
    "        layer_3 = F.max_pool1d(relu3, relu3.size(2)).squeeze(2)\n",
    "        layer_4 = F.max_pool1d(relu3, relu4.size(2)).squeeze(2)\n",
    "        \n",
    "        #print(f\"{layer_1.shape} {layer_2.shape} {layer_3.shape} {layer_4.shape}\")\n",
    "        res = self.dropout(torch.cat((layer_1,layer_2,layer_3), 1))\n",
    "\n",
    "        return self.linear(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "DRcrnLyMMJRi"
   },
   "outputs": [],
   "source": [
    "model = CustomCNN(len(TEXT.vocab),100,1,50,len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DT5ExbOZqOZ",
    "outputId": "23342a97-354d-440c-df7c-d091171bb852"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25002, 100, 1, 200, 85]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(TEXT.vocab),100,1,200,len(LABEL.vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwPa8903LmUD",
    "outputId": "e8c7da6c-f4d0-4d5d-b1c6-7462effb72c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 115 µs, sys: 3 µs, total: 118 µs\n",
      "Wall time: 123 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_ds, val_ds, test_ds), \n",
    "    batch_sizes=(32,32,32), \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "gygvSfF2LvYO"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "5b75hRmeyTXa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "Lah8EAyOMfn3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(batch.text.cuda())\n",
    "        loss = criterion(predicted_label.cpu(), batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += loss.item()\n",
    "    return total_acc / len(dataloader)\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            predicted_label = model(batch.text.cuda())\n",
    "            loss = criterion(predicted_label.cpu(), batch.label)\n",
    "            total_acc += loss.item()\n",
    "            total_count += batch.label.size(0)\n",
    "    return total_acc/total_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usigzcpNNMCQ",
    "outputId": "f66d7f70-b27f-4500-a4ca-c891158e8d61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [04:27<00:00, 19.07s/it]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 14\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(train_iterator)\n",
    "    val_loss = evaluate(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh3QIhvWwghh",
    "outputId": "fd85660e-fb95-4a96-b325-94a77d38ea82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test f1    0.120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "accu_test = evaluate(test_iterator)\n",
    "print('test f1 {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9dnUCIaqdKm"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
